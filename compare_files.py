#!/usr/bin/env python3
"""
æ¯”è¾ƒ bangumi_results.json å’Œ qb_download_rules.json æ–‡ä»¶ï¼Œåˆ†æä½œå“æ•°é‡å·®å¼‚çš„åŸå› 
"""

import json
import sys

def load_json_file(file_path):
    """åŠ è½½JSONæ–‡ä»¶"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        print(f"é”™è¯¯ï¼šæ— æ³•åŠ è½½æ–‡ä»¶ {file_path}: {e}")
        sys.exit(1)

def extract_work_names_from_bangumi(bangumi_data):
    """ä»Bangumiç»“æœä¸­æå–ä½œå“åç§°"""
    work_names = []
    for work in bangumi_data:
        # ä¼˜å…ˆä½¿ç”¨ä¸­æ–‡åç§°ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨æ¸…ç†åçš„æ—¥æ–‡æ ‡é¢˜
        if work.get('chinese_name'):
            work_names.append(work['chinese_name'])
        else:
            work_names.append(work['cleaned_title'])
    return work_names

def extract_work_names_from_qb_rules(qb_rules):
    """ä»qBittorrentè§„åˆ™ä¸­æå–ä½œå“åç§°"""
    work_names = []
    for rule_name in qb_rules.keys():
        # è§„åˆ™åç§°æ ¼å¼: "2025å¹´10æœˆæ–°ç•ª ä½œå“å"
        # å»æ‰å­£èŠ‚å‰ç¼€
        if "2025å¹´10æœˆæ–°ç•ª " in rule_name:
            work_name = rule_name.replace("2025å¹´10æœˆæ–°ç•ª ", "")
            work_names.append(work_name)
    return work_names

def analyze_differences(bangumi_names, qb_names):
    """åˆ†æä¸¤ä¸ªåˆ—è¡¨çš„å·®å¼‚"""
    bangumi_set = set(bangumi_names)
    qb_set = set(qb_names)

    print(f"\nğŸ“Š æ–‡ä»¶å¯¹æ¯”åˆ†æ")
    print(f"=" * 60)
    print(f"Bangumiç»“æœä¸­çš„ä½œå“æ•°é‡: {len(bangumi_names)}")
    print(f"qBittorrentè§„åˆ™ä¸­çš„ä½œå“æ•°é‡: {len(qb_names)}")
    print(f"å·®å¼‚æ•°é‡: {abs(len(bangumi_names) - len(qb_names))}")

    # æ‰¾å‡ºåœ¨Bangumiä¸­ä½†ä¸åœ¨qBittorrentè§„åˆ™ä¸­çš„ä½œå“
    missing_in_qb = bangumi_set - qb_set

    # æ‰¾å‡ºåœ¨qBittorrentè§„åˆ™ä¸­ä½†ä¸åœ¨Bangumiä¸­çš„ä½œå“
    extra_in_qb = qb_set - bangumi_set

    print(f"\nğŸ” è¯¦ç»†å·®å¼‚åˆ†æ")
    print(f"=" * 60)

    if missing_in_qb:
        print(f"\nâŒ åœ¨Bangumiç»“æœä¸­ä½†æœªç”ŸæˆqBittorrentè§„åˆ™çš„ä½œå“ ({len(missing_in_qb)}ä¸ª):")
        for name in sorted(missing_in_qb):
            print(f"  - {name}")
    else:
        print(f"\nâœ… æ‰€æœ‰Bangumiç»“æœéƒ½ç”Ÿæˆäº†qBittorrentè§„åˆ™")

    if extra_in_qb:
        print(f"\nâš ï¸  åœ¨qBittorrentè§„åˆ™ä¸­ä½†ä¸åœ¨Bangumiç»“æœä¸­çš„ä½œå“ ({len(extra_in_qb)}ä¸ª):")
        for name in sorted(extra_in_qb):
            print(f"  - {name}")
    else:
        print(f"\nâœ… qBittorrentè§„åˆ™ä¸­æ²¡æœ‰å¤šä½™çš„ä½œå“")

    # åˆ†æé‡å¤ä½œå“
    bangumi_duplicates = find_duplicates(bangumi_names)
    qb_duplicates = find_duplicates(qb_names)

    if bangumi_duplicates:
        print(f"\nğŸ”„ Bangumiç»“æœä¸­çš„é‡å¤ä½œå“ ({len(bangumi_duplicates)}ä¸ª):")
        for name, count in bangumi_duplicates.items():
            print(f"  - {name} (å‡ºç°{count}æ¬¡)")

    if qb_duplicates:
        print(f"\nğŸ”„ qBittorrentè§„åˆ™ä¸­çš„é‡å¤ä½œå“ ({len(qb_duplicates)}ä¸ª):")
        for name, count in qb_duplicates.items():
            print(f"  - {name} (å‡ºç°{count}æ¬¡)")

def find_duplicates(name_list):
    """æ‰¾å‡ºåˆ—è¡¨ä¸­çš„é‡å¤é¡¹"""
    from collections import Counter
    counter = Counter(name_list)
    return {name: count for name, count in counter.items() if count > 1}

def analyze_bangumi_duplicates(bangumi_data):
    """åˆ†æBangumiæ•°æ®ä¸­çš„é‡å¤é¡¹"""
    print(f"\nğŸ” Bangumiæ•°æ®è¯¦ç»†é‡å¤åˆ†æ")
    print(f"=" * 60)

    # æŒ‰ä¸­æ–‡åç§°åˆ†ç»„
    chinese_name_groups = {}
    for work in bangumi_data:
        chinese_name = work.get('chinese_name') or work['cleaned_title']
        if chinese_name not in chinese_name_groups:
            chinese_name_groups[chinese_name] = []
        chinese_name_groups[chinese_name].append(work)

    duplicates = {name: works for name, works in chinese_name_groups.items() if len(works) > 1}

    if duplicates:
        print(f"å‘ç° {len(duplicates)} ä¸ªé‡å¤ä½œå“ç»„:")
        for name, works in duplicates.items():
            print(f"\nğŸ“º ä½œå“: {name}")
            print(f"   é‡å¤æ•°é‡: {len(works)}")
            for i, work in enumerate(works, 1):
                print(f"   ç¬¬{i}ä¸ªå®ä¾‹:")
                print(f"     - åŸæ ‡é¢˜: {work['original_title']}")
                print(f"     - æ¸…ç†æ ‡é¢˜: {work['cleaned_title']}")
                print(f"     - Bangumi ID: {work.get('bangumi_id', 'æ— ')}")
                print(f"     - ä¸­æ–‡å: {work.get('chinese_name', 'æ— ')}")
                print(f"     - æ”¾é€æ—¥æœŸ: {work.get('air_date', 'æ— ')}")
    else:
        print("æœªå‘ç°é‡å¤ä½œå“")

def main():
    """ä¸»å‡½æ•°"""
    print("å¼€å§‹æ¯”è¾ƒ bangumi_results.json å’Œ qb_download_rules.json")

    # åŠ è½½æ–‡ä»¶
    bangumi_data = load_json_file('bangumi_results.json')
    qb_rules = load_json_file('qb_download_rules.json')

    print(f"æˆåŠŸåŠ è½½æ–‡ä»¶:")
    print(f"   - bangumi_results.json: {len(bangumi_data)} ä¸ªä½œå“")
    print(f"   - qb_download_rules.json: {len(qb_rules)} ä¸ªè§„åˆ™")

    # æå–ä½œå“åç§°
    bangumi_names = extract_work_names_from_bangumi(bangumi_data)
    qb_names = extract_work_names_from_qb_rules(qb_rules)

    # åˆ†æå·®å¼‚
    analyze_differences(bangumi_names, qb_names)

    # è¯¦ç»†åˆ†æBangumiä¸­çš„é‡å¤
    analyze_bangumi_duplicates(bangumi_data)

    print(f"\næ€»ç»“")
    print(f"=" * 60)
    print(f"Bangumiç»“æœæ•°é‡: {len(bangumi_data)}")
    print(f"qBittorrentè§„åˆ™æ•°é‡: {len(qb_rules)}")
    print(f"å·®å¼‚: {len(bangumi_data) - len(qb_rules)}")

    if len(bangumi_data) > len(qb_rules):
        print(f"\nåŸå› åˆ†æ:")
        print(f"  - æœ‰ {len(bangumi_data) - len(qb_rules)} ä¸ªä½œå“åœ¨Bangumiç»“æœä¸­é‡å¤")
        print(f"  - qBittorrentè§„åˆ™ç”Ÿæˆæ—¶è‡ªåŠ¨åˆå¹¶äº†é‡å¤ä½œå“")
        print(f"  - è¿™æ˜¯æ­£å¸¸è¡Œä¸ºï¼Œç¡®ä¿æ¯ä¸ªä½œå“åªæœ‰ä¸€ä¸ªä¸‹è½½è§„åˆ™")

if __name__ == "__main__":
    main()